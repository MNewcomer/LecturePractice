#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage[unicode=true]{hyperref}
\usepackage{/accounts/gen/vis/paciorek/latex/paciorek-asa,times,graphics}
\input{/accounts/gen/vis/paciorek/latex/paciorekMacros}
%\renewcommand{\baselinestretch}{1.5}
\hypersetup{unicode=true, pdfusetitle,bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true,}
\end_preamble
\use_default_options false
\begin_modules
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize letterpaper
\use_geometry true
\use_amsmath 1
\use_esint 0
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

<<setup, include=FALSE, cache=TRUE>>=
\end_layout

\begin_layout Plain Layout

## I use = but I can replace it with <-; set code/output width to be 68
\end_layout

\begin_layout Plain Layout

options(replace.assign=TRUE, width=52)
\end_layout

\begin_layout Plain Layout

@
\end_layout

\end_inset


\end_layout

\begin_layout Title
Databases and Big Data
\end_layout

\begin_layout Chunk
<<read-chunk, echo=FALSE, include=FALSE>>= 
\end_layout

\begin_layout Chunk
read_chunk('unit9-bigData.R') 
\end_layout

\begin_layout Chunk
library(ff)
\end_layout

\begin_layout Chunk
library(ffbase)
\end_layout

\begin_layout Chunk
library(spam)
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
References: 
\end_layout

\begin_layout Itemize
Murrell: Introduction to Data Technologies
\end_layout

\begin_layout Itemize
Adler: R in a Nutshell
\end_layout

\begin_layout Standard
I've also pulled material from a variety of other sources, some mentioned
 in context below.
\end_layout

\begin_layout Section
A few preparatory notes
\end_layout

\begin_layout Subsection
An editorial on 'big data'
\end_layout

\begin_layout Standard
Big data is trendy these days.
 Personally, I think some of the hype is justified and some is hype.
 Large datasets allow us to address questions that we can't with smaller
 datasets, and they allow us to consider more sophisticated (e.g., nonlinear)
 relationships than we might with a small dataset.
 But they do not directly help with the problem of correlation not being
 causation.
 Having medical data on every American still doesn't tell me if higher salt
 intake causes hypertension.
 Internet transaction data does not tell me if one website feature causes
 increased viewership or sales.
 One either needs to carry out a designed experiment or think carefully
 about how to infer causation from observational data.
 Nor does big data help with the problem that an ad hoc 'sample' is not
 a statistical sample and does not provides the ability to directly infer
 properties of a population.
 A well-chosen smaller dataset may be much more informative than a much
 larger, more ad hoc dataset.
 However, having big datasets might allow you to select from the dataset
 in a way that helps get at causation or in a way that allows you to construct
 a population-representative sample.
\end_layout

\begin_layout Standard
Different people define the 'big' in big data differently.
 Our efforts here will focus on dataset sizes that are large for traditional
 statistical work but would probably not be thought of as large in some
 contexts such as Google or the NSA.
\end_layout

\begin_layout Subsection
Logistics
\end_layout

\begin_layout Standard
One of the main drawbacks with R in working with big data is that all objects
 are stored in memory, so you can't directly work with datasets that are
 more than 1-20 Gb or so, depending on the memory on your machine.
 
\end_layout

\begin_layout Standard
Note: in handling big data files, it's best to have the data on the local
 disk of the machine you are using to reduce traffic and delays from moving
 data over the network.
\end_layout

\begin_layout Subsection
What we already know about handling big data!
\end_layout

\begin_layout Standard
UNIX operations are generally very fast, so if you can manipulate your data
 via UNIX commands and piping, that will allow you to do a lot.
 We've already seen UNIX commands for extracting columns.
 And various commands such as 
\emph on
grep
\emph default
, 
\emph on
head
\emph default
, 
\emph on
tail
\emph default
, etc.
 allow you to pick out rows based on certain criteria.
 As some of you have done in problem sets, one can use 
\emph on
awk
\emph default
 to extract rows.
 So basic shell scripting may allow you to reduce your data to a more manageable
 size.
 
\end_layout

\begin_layout Standard
Also, the example datasets in Section 3 are not good illustrations of this,
 but as we'll see scattered throughout the Unit, there are more compact
 ways of storing data than in flat text (e.g., csv) files.
 
\end_layout

\begin_layout Section
Databases
\end_layout

\begin_layout Subsection
Overview
\end_layout

\begin_layout Standard
A relational database stores data as a set of tables (or relations), which
 are rather similar to R data frames, in that a table is made up of columns
 or fields, each containing a single type (numeric, character, date, currency,
 ...) and rows or records containing the observations for one entity.
 One principle of databases is that if a category is repeated in a given
 variable, you can more efficiently store information about each level of
 the category in a separate table; consider information about people living
 in a state and information about each state - you don't want to include
 variables that only vary by state in the table containing information about
 individuals (at least until you're doing the actual analysis that needs
 the information in a single table).
 Or consider students nested within classes nested within schools.
 Databases are set up to allow for fast querying and merging (called 
\emph on
joins
\emph default
 in database terminology).
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
[show picture on p.
 122 of ItDT]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
You can interact with databases in a variety of database systems (DBMS=database
 management system) (some systems are 
\emph on
SQLite
\emph default
, 
\emph on
MySQL
\emph default
, 
\emph on
postgreSQL
\emph default
, 
\emph on
Oracle
\emph default
, 
\emph on
Access
\emph default
).
 We'll concentrate on accessing data in a database rather than management
 of databases.
 SQL is the 
\emph on
Structured Query Language
\emph default
 and is a special-purpose language for managing databases and making queries.
 Variations on SQL are used in many different DBMS.
\end_layout

\begin_layout Standard
Many DBMS have a client-server model.
 Clients connect to the server, with some authentication, and make requests.
 We'll concentrate here on a simple DBMS, 
\emph on
SQLite
\emph default
, that allows us to just work on our local machine, with the database stored
 as a single file.
\end_layout

\begin_layout Standard
There are often multiple ways to interact with a DBMS, including directly
 using command line tools provided by the DBMS or via Python or R, among
 others.
 
\end_layout

\begin_layout Standard
We'll use an SQLite database available on any SCF machine at 
\emph on
/mirror/data/pub/html/scf/cis.db
\emph default
 as our example database.
 This is a database of the metadata (authors, titles, years, journal, etc.)
 for articles published in Statistics journals over the last century.
 First, let's talk through how one would set up a relational database to
 store journal article information.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
[have them chime in with the table setup - journal info, author info, article
 info]
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Accessing databases in R
\end_layout

\begin_layout Standard
In R, the 
\emph on
DBI
\emph default
 package provides a front-end for manipulating databases from a variety
 of DBMS (MySQL, SQLite, Oracle, among others).
 Basically, you tell the package what DBMS is being used on the backend,
 link to the actual database, and then you can use the syntax in the package.
 
\end_layout

\begin_layout Standard
First we'll connect to the database and get some information on the 
\emph on
schema
\emph default
, i.e., the structure of the database.
\end_layout

\begin_layout Chunk
<<chunk1>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
For queries, SQL has statements like:
\end_layout

\begin_layout Standard

\family typewriter
SELECT var1, var2, var3 FROM tableX WHERE condition1 AND condition2 ORDER
 BY var4
\family default

\begin_inset Newline newline
\end_inset

E.g., 
\emph on
condition1
\emph default
 might be 
\family typewriter
latitude > 80
\family default
 or 
\family typewriter
name = 'Breiman'
\family default
 or 
\family typewriter
company in ('IBM', 'Apple', 'Dell')
\family default
.
 Now we'll do some queries to pull together information we want.
 Because of the relational structure, to extract the titles for a given
 author, we need to do a series of queries.
 
\end_layout

\begin_layout Chunk
<<chunk2, tidy=FALSE>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Note that we were able to insert values from R into the set used to do the
 selection.
\end_layout

\begin_layout Standard
Now let's see a 
\emph on
join
\emph default
 (by default this is an 
\begin_inset Quotes eld
\end_inset


\emph on
inner join
\emph default

\begin_inset Quotes erd
\end_inset

 -- see below) of multiple tables, combined with a query.
 This allows us to extract the information on Breiman's articles more easily.
\end_layout

\begin_layout Chunk
<<chunk3>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Finally, let's see the idea of creating a 
\emph on
view
\emph default
, which you can think of as a new table, though the DBMS is not actually
 explicitly constructing such a table.
\end_layout

\begin_layout Chunk
<<chunk4>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
As seen above, you can also use 
\emph on
dbSendQuery()
\emph default
 combined with 
\emph on
fetch()
\emph default
 to pull in a fixed number of records at a time, if you're working with
 a big database.
 
\end_layout

\begin_layout Subsection
Details on joins
\end_layout

\begin_layout Standard
A bit more on joins - as we saw with 
\emph on
merge()
\emph default
 in R, there are various possibilities for how to do the merge depending
 on whether there are rows in one table that are not in another table.
 In other words, we need to think about whether the relationship between
 tables is one-to-one, one-to-many, or many-to-many.
 In database terminology an 
\emph on
inner join
\emph default
 is when you get the rows for which there is data in both tables.
 A 
\emph on
left outer join
\emph default
 gives all the rows from the first table but only those from the second
 table that match a row in the first table.
 A 
\emph on
right outer join
\emph default
 is the reverse, while a 
\emph on
full outer join
\emph default
 returns all rows from both tables.
 A 
\emph on
cross join
\emph default
 gives the Cartesian product, namely the combination of every row from each
 table, analogous to 
\emph on
expand.grid()
\emph default
 in R.
 However a 
\emph on
cross join
\emph default
 with a 
\emph on
where
\emph default
 statement can duplicate the result of an 
\emph on
inner join
\emph default
:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

select * from table1 cross join table2 where table1.id = table2.id
\end_layout

\begin_layout Plain Layout

select * from table1 join table2 on table1.id = table2.id
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Keys and indices
\end_layout

\begin_layout Standard
A key is a field or collection of fields that gives a unique value for every
 row/observation.
 A table in a database should then have a primary key that is the main unique
 identifier used by the DBMS.
 Foreign keys are columns in one table that give the value of the primary
 key in another table.
\end_layout

\begin_layout Standard
An index is an ordering of rows based on one or more fields.
 DBMS use indices to look up values quickly.
 (Recall our discussion in Unit 6 on looking up values by name vs.
 index and the benefits of hashing.) So in general you want your tables to
 have indices.
 And having indices on the columns used in the matching for a join allows
 for quick joins.
 DBMS use indexing to provide sub-linear time lookup, so that lookup is
 faster than linear time (
\begin_inset Formula $O(n)$
\end_inset

 when there are 
\begin_inset Formula $n$
\end_inset

 rows), which is what would occur if one had to look at each row sequentially.
 Lookup may be logarithmic [
\begin_inset Formula $O(log(n))$
\end_inset

] or constant time [
\begin_inset Formula $O(1)$
\end_inset

].
 A binary search is logarithmic while looking up based on numeric position
 is 
\begin_inset Formula $O(1)$
\end_inset

.
\end_layout

\begin_layout Standard
So if you're working with a database and speed is important, check to see
 if there are indices.
\end_layout

\begin_layout Subsection
Creating SQLite database tables from R
\end_layout

\begin_layout Standard
I won't do a full demo of this, but the basic syntax for this is as follows.
 You can read from a CSV to create the table or from an R dataframe.
 The following assumes you have two tables stored as CSVs, with one table
 of student info and one table of class info.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

dbWriteTable(conn = db, name = "student", value = "student.csv",
\end_layout

\begin_layout Plain Layout

   row.names = FALSE, header = TRUE)
\end_layout

\begin_layout Plain Layout

dbWriteTable(conn = db, name = "class", value = "class.csv",
\end_layout

\begin_layout Plain Layout

   row.names = FALSE, header = TRUE)
\end_layout

\begin_layout Plain Layout

# alternatively
\end_layout

\begin_layout Plain Layout

school <- read.csv("school.csv") # Read csv files into R
\end_layout

\begin_layout Plain Layout

class <- read.csv("class.csv")
\end_layout

\begin_layout Plain Layout

# Import data frames into database
\end_layout

\begin_layout Plain Layout

dbWriteTable(conn = db, name = "student", value = student, 
\end_layout

\begin_layout Plain Layout

   row.names = FALSE)
\end_layout

\begin_layout Plain Layout

dbWriteTable(conn = db, name = "class", value = class, 
\end_layout

\begin_layout Plain Layout

   row.names = FALSE)
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
SAS
\end_layout

\begin_layout Standard
SAS is quite good at handling large datasets, storing them on disk rather
 than in memory.
 I have used SAS in the past for subsetting and merging large datasets.
 Then I will generally extract the data I need for statistical modeling
 and do the analysis in R.
 
\end_layout

\begin_layout Standard
Here's an example of some SAS code for reading in a CSV followed by some
 subsetting and merging and then output.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

/* we can use a pipe - in this case to remove carriage returns, */
\end_layout

\begin_layout Plain Layout

/* presumably because the CSV file was created in Windows */
\end_layout

\begin_layout Plain Layout

filename tmp pipe "cat ~/shared/hei/gis/100w4kmgrid.csv | tr -d '
\backslash
r'";
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

/* read in one data file */
\end_layout

\begin_layout Plain Layout

data grid;
\end_layout

\begin_layout Plain Layout

	infile tmp
\end_layout

\begin_layout Plain Layout

	lrecl=500 truncover dsd firstobs=2; 
\end_layout

\begin_layout Plain Layout

	informat gridID x y landMask dataMask;
\end_layout

\begin_layout Plain Layout

	input gridID x y landMask dataMask;
\end_layout

\begin_layout Plain Layout

run ;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

filename tmp pipe "cat ~/shared/hei/GOES12/goes/Goes_int4km.csv | tr -d '
\backslash
r'";
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

/* read in second data file */
\end_layout

\begin_layout Plain Layout

data match;
\end_layout

\begin_layout Plain Layout

	infile tmp
\end_layout

\begin_layout Plain Layout

	lrecl=500 truncover dsd firstobs=2; 
\end_layout

\begin_layout Plain Layout

	informat goesID gridID areaInt areaPix;
\end_layout

\begin_layout Plain Layout

	input goesID gridID areaInt areaPix;
\end_layout

\begin_layout Plain Layout

run ;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

/* need to sort before merging */
\end_layout

\begin_layout Plain Layout

proc sort data=grid;
\end_layout

\begin_layout Plain Layout

    by gridID;
\end_layout

\begin_layout Plain Layout

run;
\end_layout

\begin_layout Plain Layout

proc sort data=match;
\end_layout

\begin_layout Plain Layout

    by gridID;
\end_layout

\begin_layout Plain Layout

run;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

/* notice some similarity to SQL */
\end_layout

\begin_layout Plain Layout

data merged;
\end_layout

\begin_layout Plain Layout

	merge match(in=in1) grid(in=in2);
\end_layout

\begin_layout Plain Layout

	by gridID;  /* key field */
\end_layout

\begin_layout Plain Layout

	if in1=1;   /* also do some subsetting */
\end_layout

\begin_layout Plain Layout

	/* only keep certain fields */
\end_layout

\begin_layout Plain Layout

	keep gridID goesID x y landMask dataMask areaInt areaPix;
\end_layout

\begin_layout Plain Layout

run;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

/* do some subsetting */
\end_layout

\begin_layout Plain Layout

data PA;   /* new dataset */
\end_layout

\begin_layout Plain Layout

    set merged;  /* original dataset */
\end_layout

\begin_layout Plain Layout

    if x<1900000 and x>1200000 and y<2300000 and y>1900000;
\end_layout

\begin_layout Plain Layout

run;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%let filename="~/shared/hei/code/model/GOES-gridMatchPA.csv";
\end_layout

\begin_layout Plain Layout

/* output to CSV */
\end_layout

\begin_layout Plain Layout

PROC EXPORT DATA= WORK.PA
\end_layout

\begin_layout Plain Layout

            OUTFILE= &filename
\end_layout

\begin_layout Plain Layout

            DBMS=CSV REPLACE;
\end_layout

\begin_layout Plain Layout

RUN;
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that SAS is oriented towards working with data in a 
\begin_inset Quotes eld
\end_inset

data frame
\begin_inset Quotes erd
\end_inset

-style format; i.e., rows as observations and columns as fields, with different
 fields of possibly different types.
 As you can see in the syntax above, the operations concentrate on transforming
 one dataset into another dataset.
 
\end_layout

\begin_layout Section
R and big data 
\end_layout

\begin_layout Standard
There has been a lot of work in recent years to allow R to work with big
 datasets.
\end_layout

\begin_layout Standard
The 
\emph on
ff
\emph default
 and 
\emph on
bigmemory
\emph default
 packages provide the ability to load datasets into R without having them
 in memory, but rather stored in clever ways on disk that allow for fast
 access.
 Metadata is stored in R.
 
\end_layout

\begin_layout Standard
The 
\emph on
biglm
\emph default
 package provides the ability to fit linear models and GLMs to big datasets,
 with integration with 
\emph on
ff
\emph default
 and 
\emph on
bigmemory
\emph default
.
\end_layout

\begin_layout Subsection
Working with big datasets on disk: ff and bigmemory
\end_layout

\begin_layout Standard
We'll work through an example with US government data on airline delays
 (1987-2008) available through the ASA 2009 Data Expo at 
\begin_inset CommandInset href
LatexCommand href
target "http://stat-computing.org/dataexpo/2009/the-data.html"

\end_inset

.
\end_layout

\begin_layout Standard
First we'll use UNIX tools to download the individual yearly CSV files and
 make a single CSV (~12 Gb).
 See the demo code file for the bash code.
\end_layout

\begin_layout Standard
Now we can read the data into R using the 
\emph on
ff
\emph default
 package, in particular reading in as an 
\emph on
ffdf
\emph default
 object.
 Note the arguments are similar to those for 
\emph on
read.{table,csv}()
\emph default
.
 
\emph on
read.table.ffdf()
\emph default
 reads the data in chunks.
\end_layout

\begin_layout Chunk
<<ff, eval=FALSE, tidy=FALSE>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
We can write a copy of the file in the ff binary format that can be read
 more quickly back into R than the original reading of the CSV using 
\emph on
ffsave()
\emph default
 and 
\emph on
ffload()
\emph default
.
 Also note the reduced size of the binary format file compared to the original
 CSV.
 It's good to be aware of where the binary ff file is stored.
 With 
\emph on
ff
\emph default
 (I think 
\emph on
bigmemory
\emph default
 is different in how it handles this) it appears to be stored in 
\emph on
/tmp
\emph default
 in an R temporary directory.
 Note that as we work with large files we need to be more aware of the filesyste
m, making sure in this case that 
\emph on
/tmp
\emph default
 has enough space.
 
\end_layout

\begin_layout Standard
Let's look at the ff and ffbase packages to see what functions are available
 using 
\family typewriter
library(help=ff)
\family default
.
 Notice that there is an 
\emph on
merge.ff()
\emph default
.
\end_layout

\begin_layout Standard
Note that a copy of an 
\emph on
ff
\emph default
 object appears to be a shallow copy.
\end_layout

\begin_layout Standard
Next let's do a bit of exploration of the dataset.
 Of course in a real analysis we'd do a lot more and some of this would
 take some time.
\end_layout

\begin_layout Chunk
<<tableInfo, cache=TRUE, eval=FALSE, fig.width=4>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
A note of caution.
 Debugging code involving 
\emph on
ff
\emph default
 can be a hassle because the size gets in the way in various ways.
 Until you're familiar with the various operations on ff objects, you'd
 be wise to try to run your code on a small test dataset loaded in as an
 ff object.
 Also, we want to be sure that the operations we use keep any resulting
 large objects in the 
\emph on
ff
\emph default
 format and use 
\emph on
ff
\emph default
 methods and not standard R functions.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
First, following the steps outlined at http://www.bigmemory.org/, we'll download
 the individual yearly CSV files and make a single CSV (~12 Gb).
 Then we'll use the python script they provide to format the data, which
 produces airline.csv, which is X Gb.
 Unfortunately, bigmemory requires a CSV as input, so we can't read from
 a bz2 file via a file connection.
 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Fitting models to big datasets: biglm
\end_layout

\begin_layout Standard
The 
\emph on
biglm
\emph default
 package provides the ability to fit large linear models and GLMs.
 
\emph on
ffbase
\emph default
 has a 
\emph on
bigglm.ffdf()
\emph default
 function that builds on 
\emph on
biglm
\emph default
 for use with ffd objects.
 Let's try a few basic models on the airline data.
\end_layout

\begin_layout Chunk
<<airline-model, eval=FALSE, tidy=FALSE>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Given the time involved, I ran that code separately from compiling the pdf.
 Here are the results:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

# basic model
\end_layout

\begin_layout Plain Layout

Large data regression model: bigglm(DepDelay ~ Year, data = datUse)
\end_layout

\begin_layout Plain Layout

Sample size =  121216293 
\end_layout

\begin_layout Plain Layout

                 Coef      (95%       CI)     SE p
\end_layout

\begin_layout Plain Layout

(Intercept) -286.7616 -288.3136 -285.2096 0.7760 0
\end_layout

\begin_layout Plain Layout

Year           0.1475    0.1468    0.1483 0.0004 0
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

# expanded model
\end_layout

\begin_layout Plain Layout

Large data regression model: bigglm(DepDelay ~ Year + Month + DayOfWeek,
 data = dat)
\end_layout

\begin_layout Plain Layout

Sample size =  123534969 
\end_layout

\begin_layout Plain Layout

                 Coef      (95%       CI)     SE p
\end_layout

\begin_layout Plain Layout

(Intercept) -286.7663 -288.4059 -285.1266 0.8198 0
\end_layout

\begin_layout Plain Layout

Year           0.1464    0.1456    0.1472 0.0004 0
\end_layout

\begin_layout Plain Layout

Month11        0.8273    0.8026    0.8520 0.0124 0
\end_layout

\begin_layout Plain Layout

Month12        5.2685    5.2439    5.2931 0.0123 0
\end_layout

\begin_layout Plain Layout

Month1         3.1617    3.1368    3.1865 0.0124 0
\end_layout

\begin_layout Plain Layout

Month2         2.6968    2.6714    2.7222 0.0127 0
\end_layout

\begin_layout Plain Layout

Month3         2.3926    2.3679    2.4172 0.0123 0
\end_layout

\begin_layout Plain Layout

Month4         0.5518    0.5270    0.5767 0.0124 0
\end_layout

\begin_layout Plain Layout

Month5         0.5853    0.5606    0.6100 0.0123 0
\end_layout

\begin_layout Plain Layout

Month6         3.9741    3.9493    3.9989 0.0124 0
\end_layout

\begin_layout Plain Layout

Month7         3.5967    3.5721    3.6212 0.0123 0
\end_layout

\begin_layout Plain Layout

Month8         2.5625    2.5380    2.5870 0.0123 0
\end_layout

\begin_layout Plain Layout

Month9        -0.9423   -0.9673   -0.9173 0.0125 0
\end_layout

\begin_layout Plain Layout

DayOfWeek2    -0.9901   -1.0090   -0.9712 0.0095 0
\end_layout

\begin_layout Plain Layout

DayOfWeek3    -0.2004   -0.2193   -0.1815 0.0094 0
\end_layout

\begin_layout Plain Layout

DayOfWeek4     1.3871    1.3682    1.4059 0.0094 0
\end_layout

\begin_layout Plain Layout

DayOfWeek5     2.2921    2.2732    2.3110 0.0094 0
\end_layout

\begin_layout Plain Layout

DayOfWeek6    -0.9619   -0.9814   -0.9424 0.0098 0
\end_layout

\begin_layout Plain Layout

DayOfWeek7     0.5449    0.5258    0.5641 0.0096 0
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Of course as good statisticians/data analysts we want to do careful assessment
 of our model, consideration of alternative models, etc.
 This is going to be harder to do with large datasets than with more manageable
 ones.
 However, one possibility is to do the diagnostic work on subsamples of
 the data.
\end_layout

\begin_layout Standard
Now let's consider the fact that very small substantive effects can be highly
 statistically significant when estimated from a large dataset.
 In this analysis the data are generated from 
\begin_inset Formula $Y\sim\mathcal{N}(0+0.001x,1)$
\end_inset

, so the 
\begin_inset Formula $R^{2}$
\end_inset

 is essentially zero.
\end_layout

\begin_layout Chunk
<<significance-prep, eval=FALSE, tidy=FALSE>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Chunk
<<significance-model, eval=FALSE, tidy=FALSE>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
Given the time involved, I ran that code separately from compiling the pdf.
 Here are the results:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

Large data regression model: bigglm(y ~ x1 + x2 + x3, data = dat)
\end_layout

\begin_layout Plain Layout

Sample size = 1.5e+08 
\end_layout

\begin_layout Plain Layout

              Coef         (95%       CI)          SE         p
\end_layout

\begin_layout Plain Layout

(Intercept) -0.0001437 -0.0006601 0.0003727 0.0002582 0.5777919
\end_layout

\begin_layout Plain Layout

x1           0.0013703  0.0008047 0.0019360 0.0002828 0.0000013
\end_layout

\begin_layout Plain Layout

x2           0.0002371 -0.0003286 0.0008028 0.0002828 0.4018565
\end_layout

\begin_layout Plain Layout

x3          -0.0002620 -0.0008277 0.0003037 0.0002829 0.3542728
\end_layout

\begin_layout Plain Layout

### and here is the R^2 calculation (why can it be negative?)
\end_layout

\begin_layout Plain Layout

[1] -1.111046828e-06
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
So, do I care the result is highly significant? Perhaps if I'm hunting the
 Higgs boson...
 As you have hopefully seen in statistics courses, statistical significance
 
\begin_inset Formula $\ne$
\end_inset

 practical significance.
\end_layout

\begin_layout Section
Sparsity
\end_layout

\begin_layout Standard
A lot of statistical methods are based on sparse matrices.
 These include:
\end_layout

\begin_layout Itemize
Matrices representing the neighborhood structure (i.e., conditional dependence
 structure) of networks/graphs.
\end_layout

\begin_layout Itemize
Matrices representing autoregressive models (neighborhood structure for
 temporal and spatial data)
\end_layout

\begin_layout Itemize
A statistical method called the 
\emph on
lasso
\emph default
 is used in high-dimensional contexts to give sparse results (sparse parameter
 vector estimates, sparse covariance matrix estimates)
\end_layout

\begin_layout Itemize
There are many others (I've been lazy here in not coming up with a comprehensive
 list, but trust me!)
\end_layout

\begin_layout Standard
When storing and manipulating sparse matrices, there is no need to store
 the zeros, nor to do any computation with elements that are zero.
\end_layout

\begin_layout Standard
R, Matlab and Python all have functionality for storing and computing with
 sparse matrices.
 We'll see this a bit more in the linear algebra unit.
\end_layout

\begin_layout Chunk
<<spam, cache=TRUE>>=
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Section
Using statistical concepts to deal with computational bottlenecks
\end_layout

\begin_layout Standard
As statisticians, we have a variety of tools that can aid in dealing with
 big data.
\end_layout

\begin_layout Enumerate
Usually we take samples because we cannot collect data on the entire population.
 But we can just as well take a sample because we don't have the ability
 to process the data from the entire population.
 We can use standard uncertainty estimates to tell us how close to the true
 quantity we are likely to be.
 And we can always take a bigger sample if we're not happy with the amount
 of uncertainty.
\end_layout

\begin_layout Enumerate
There are a variety of ideas out there for making use of sampling to address
 big data challenges.
 One idea (due in part to Prof.
 Mike Jordan here in Statistics/EECS) is to compute estimates on many (relativel
y small) bootstrap samples from the data (cleverly creating a reduced-form
 version of the entire dataset from each bootstrap sample) and then combine
 the estimates across the samples.
 Here's 
\begin_inset CommandInset href
LatexCommand href
name "the arXiv paper"
target "http://arxiv.org/abs/1112.5016"

\end_inset

 on this topic.
\end_layout

\begin_layout Enumerate
Randomized algorithms: there has been a lot of attention recently to algorithms
 that make use of randomization.
 E.g., in optimizing a likelihood, you might choose the next step in the optimizat
ion based on random subset of the data rather than the full data.
 Or in a regression context you might choose a subset of rows of the design
 matrix (the matrix of covariates) and corresponding observations, weighted
 based on the statistical leverage [recall the discussion of regression
 diagnostics in a regression course] of the observations.
 Here's another 
\begin_inset CommandInset href
LatexCommand href
name "arXiv paper"
target "http://arxiv.org/abs/1104.5557"

\end_inset

 that provides some ideas in this area.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
There are standard estimators (e.g., from the literature on imputation for
 missing data) for doing this combination that account for the within sample
 uncertainty and the across-sample variability.
 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Hadoop and MapReduce
\end_layout

\begin_layout Subsection
Overview
\end_layout

\begin_layout Standard
A basic paradigm for working with big datasets is the 
\emph on
MapReduce
\emph default
 paradigm.
 The basic idea is to store the data in a distributed fashion across multiple
 nodes and try to do the computation in pieces on the data on each node.
 Results can also be stored in a distributed fashion.
\end_layout

\begin_layout Standard
The basic steps of 
\emph on
MapReduce
\emph default
 are as follows:
\end_layout

\begin_layout Itemize
read individual data objects (e.g., records/lines from CSVs or individual
 data files)
\end_layout

\begin_layout Itemize
map: create key-value pairs using the inputs (more formally, the map step
 takes a key-value pair and returns a new key-value pair)
\end_layout

\begin_layout Itemize
reduce - for each key, do an operation on the associated values and create
 a result - i.e., aggregate within the values assigned to each key
\end_layout

\begin_layout Itemize
write out the {key,result} pair
\end_layout

\begin_layout Standard
A similar paradigm that is being implemented in some R packages by Hadley
 Wickham is the split-apply-combine strategy (
\begin_inset CommandInset href
LatexCommand href
target "http://www.jstatsoft.org/v40/i01/paper"

\end_inset

).
\end_layout

\begin_layout Standard

\emph on
Hadoop
\emph default
 is an infrastructure for enabling MapReduce across a network of machines.
 The basic idea is to hide the complexity of distributing the calculations
 and collecting results.
 Hadoop includes a file system for distributed storage (HDFS), where each
 piece of information is stored redundantly (on multiple machines).
 Calculations can then be done in a parallel fashion, often on data in place
 on each machine thereby limiting the amount of communication that has to
 be done over the network.
 Hadoop also monitors completion of tasks and if a node fails, it will redo
 the relevant tasks on another node.
 Hadoop is based on Java but there are projects that allow R to interact
 with Hadoop, in particular 
\emph on
RHadoop
\emph default
 and 
\emph on
RHipe
\emph default
.
 Rhadoop provides the 
\emph on
rmr
\emph default
, 
\emph on
rhdfs
\emph default
, and 
\emph on
rhbase
\emph default
 packages.
 For more details on RHadoop see Adler and 
\begin_inset CommandInset href
LatexCommand href
target "http://blog.revolutionanalytics.com/2011/09/mapreduce-hadoop-r.html"

\end_inset

.
\end_layout

\begin_layout Standard
Setting up a Hadoop cluster can be tricky.
 We have a basic test setup on the SCF (
\begin_inset CommandInset href
LatexCommand href
target "statistics.berkeley.edu/computing/hadoop"

\end_inset

), but it's not meant for serious calculations.
 Hopefully if you're in a position to need to use Hadoop, it will be set
 up for you and you will be interacting with it as a user/data analyst.
\end_layout

\begin_layout Subsection
MapReduce and RHadoop
\end_layout

\begin_layout Standard
Let's see some examples of the MapReduce approach using R syntax of the
 sort one would use with 
\emph on
RHadoop
\emph default
.
 While we'll use R syntax in some cases, but the basic idea of what the
 map and reduce functions are is not specific to R.
 Note that using Hadoop with R may be rather slower than actually writing
 Java code for Hadoop.
\end_layout

\begin_layout Standard
First, let's consider a basic word-counting example.
 Suppose we have many, many individual text documents distributed as individual
 files in the HDFS.
 Here's pseudo code from Wikipedia.
 Here in the map function, the input {key,value} pair is the name of a document
 and the words in the document and the output {key, value} pairs are each
 word and the value 1.
 Then the reduce function takes each key (i.e., each word) and counts up the
 number of ones.
 The output {key, value} pair from the reduce step is the word and the count
 for that word.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

function map(String name, String document):
\end_layout

\begin_layout Plain Layout

// name (key): document name
\end_layout

\begin_layout Plain Layout

// document (value): document contents
\end_layout

\begin_layout Plain Layout

   for each word w in document:
\end_layout

\begin_layout Plain Layout

      return (w, 1) 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

function reduce(String word, Iterator partialCounts):
\end_layout

\begin_layout Plain Layout

// word (key): a word
\end_layout

\begin_layout Plain Layout

// partialCounts (values): a list of aggregated partial counts
\end_layout

\begin_layout Plain Layout

sum = 0
\end_layout

\begin_layout Plain Layout

for each pc in partialCounts:
\end_layout

\begin_layout Plain Layout

   sum += pc
\end_layout

\begin_layout Plain Layout

return (word, sum)
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now let's consider an example where we calculate mean and standard deviation
 for the income of individuals in each state.
 Assume we have a large collection of CSVs, with each row containing information
 on an individual.
 
\emph on
mapreduce()
\emph default
 and 
\emph on
keyval()
\emph default
 are functions in the RHadoop package.
 I'll assume we've written a separate helper function, 
\emph on
my_readline()
\emph default
, that manipulates individual lines from the CSVs.
\end_layout

\begin_layout Chunk
<<mr-example, eval=FALSE, tidy=FALSE>>=
\end_layout

\begin_layout Chunk
library(rmr)
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
mymap <- function(k, v) {
\end_layout

\begin_layout Chunk
   record <- my_readline(v)
\end_layout

\begin_layout Chunk
   key <- record[['state']]
\end_layout

\begin_layout Chunk
   value <- record[['income']]
\end_layout

\begin_layout Chunk
   keyval(key, value)
\end_layout

\begin_layout Chunk
}
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
myreduce <- function(k, v){
\end_layout

\begin_layout Chunk
   keyval(k, c(length(v), mean(v), sd(v)))
\end_layout

\begin_layout Chunk
}
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
incomeResults <- mapreduce(
\end_layout

\begin_layout Chunk
   input = "incomeData",
\end_layout

\begin_layout Chunk
   map = mymap,
\end_layout

\begin_layout Chunk
   reduce = myreduce,
\end_layout

\begin_layout Chunk
   combine = NULL,
\end_layout

\begin_layout Chunk
   input.format = 'csv',
\end_layout

\begin_layout Chunk
   output.format = 'csv')
\end_layout

\begin_layout Chunk

\end_layout

\begin_layout Chunk
from.dfs(incomeResults, format = 'csv', structured = TRUE)
\end_layout

\begin_layout Chunk
@
\end_layout

\begin_layout Standard
A few additional comments.
 In our map function, we could exclude values or transform them in some
 way, including producing multiple records from a single record.
 And in our reduce function, we can do more complicated analysis.
 So one can actually do fairly sophisticated things within what may seem
 like a restrictive paradigm.
 But we are constrained such that in the map step, each record needs to
 be treated independently and in the reduce step each key needs to be treated
 independently.
 This allows for the parallelization.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
stratified analysis
\end_layout

\begin_layout Plain Layout
bag little bootstraps - assign each obs to bootstrap sample or not to any
\end_layout

\end_inset


\end_layout

\end_body
\end_document
